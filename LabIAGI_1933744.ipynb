{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Apertura dei dati da Google Drive\n",
        "#Caricamento dei file testuali per navigare nelle cartelle e prendere le immagini\n",
        "file=open(\"./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/train.txt\", \"r\")\n",
        "content = file.read()\n",
        "train_set = content.strip().split(\",\")\n",
        "print(train_set)\n",
        "file.close()\n",
        "\n",
        "file=open(\"./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/test.txt\", \"r\")\n",
        "content = file.read()\n",
        "test_set = content.strip().split(\",\")\n",
        "print(test_set)\n",
        "file.close()\n",
        "\n",
        "\n",
        "file=open(\"./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/all.txt\", \"r\")\n",
        "content = file.read()\n",
        "all_locs = content.strip().split(\",\")\n",
        "print(all_locs)\n",
        "file.close()\n",
        "\n",
        "channels = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']"
      ],
      "metadata": {
        "id": "InYkpqTZmqjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78394c5e-4b1c-4cb6-ed6b-1d5cd0f332c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aguasclaras', 'bercy', 'bordeaux', 'nantes', 'paris', 'rennes', 'saclay_e', 'abudhabi', 'cupertino', 'pisa', 'beihai', 'hongkong', 'beirut', 'mumbai']\n",
            "['brasilia', 'montpellier', 'norcia', 'rio', 'saclay_w', 'valencia', 'dubai', 'lasvegas', 'milano', 'chongqing']\n",
            "['aguasclaras', 'bercy', 'bordeaux', 'nantes', 'paris', 'rennes', 'saclay_e', 'abudhabi', 'cupertino', 'pisa', 'beihai', 'hongkong', 'beirut', 'mumbai', 'brasilia', 'montpellier', 'norcia', 'rio', 'saclay_w', 'valencia', 'dubai', 'lasvegas', 'milano', 'chongqing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/abudhabi/imgs_1_rect/B01.tif')\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "tensor=F.to_tensor(img)\n",
        "print(tensor)\n",
        "\n",
        "print(img)\n",
        "\n"
      ],
      "metadata": {
        "id": "8qZt_DMwrFhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe2d11d-a6e2-48b5-a60b-6535746ce485"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[2123, 2115, 2096,  ..., 2368, 2363, 2359],\n",
            "         [2124, 2116, 2097,  ..., 2367, 2363, 2360],\n",
            "         [2127, 2119, 2100,  ..., 2366, 2363, 2360],\n",
            "         ...,\n",
            "         [2255, 2254, 2250,  ..., 2415, 2431, 2440],\n",
            "         [2264, 2262, 2258,  ..., 2407, 2423, 2433],\n",
            "         [2264, 2262, 2258,  ..., 2407, 2423, 2433]]], dtype=torch.int16)\n",
            "<PIL.TiffImagePlugin.TiffImageFile image mode=I;16 size=785x799 at 0x7F462EF07F10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from math import floor, ceil, sqrt, exp\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "class RandomFlip():\n",
        "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        if random.random() > 0.5:\n",
        "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  label.numpy()[:,::-1].copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "class RandomRot():\n",
        "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        n = random.randint(0, 3)\n",
        "        if n:\n",
        "            I1 =  sample['I1'].numpy()\n",
        "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  sample['I2'].numpy()\n",
        "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  sample['label'].numpy()\n",
        "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    # your code here\n",
        "    RandomFlip(),\n",
        "    RandomRot()\n",
        "])\n",
        "\n",
        "\n",
        "#Inizilizzazione del dataset\n",
        "class OSCD_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, train=True, normalize=True, transform=transform, patch_side = 96, stride = None):\n",
        "        self.train=train\n",
        "        self.transform=transform\n",
        "        self.normalize=normalize\n",
        "        self.patch_side=patch_side\n",
        "        self.stride=1\n",
        "        if stride:\n",
        "          self.stride=stride\n",
        "        #parametro utilizzato da quelli del paper per\n",
        "        #aumentare il peso dei changed pixel.\n",
        "        #potrebbe essere utile farne il tuning (?)\n",
        "        self.CIBW=10\n",
        "        #Carica i dati dai file\n",
        "        self.load_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Restituisci la lunghezza del dataset\n",
        "        return self.n_patches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        current_patch_coords = self.patch_coords[idx]\n",
        "        loc = current_patch_coords[0]\n",
        "        limits = current_patch_coords[1]\n",
        "\n",
        "        I1 = self.imgs_1[loc][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        I2 = self.imgs_2[loc][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        label = self.change_maps[loc][limits[0]:limits[1], limits[2]:limits[3]]\n",
        "      \n",
        "\n",
        "        I1 = torch.from_numpy(I1)\n",
        "        I2 = torch.from_numpy(I2)\n",
        "        label = torch.from_numpy(label).float()\n",
        "\n",
        "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \n",
        "        return sample\n",
        "    \n",
        "    def get_img(self, loc):\n",
        "       return self.imgs_1[loc], self.imgs_2[loc], self.change_maps[loc]\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "      #selezione del sottoinsieme richiesto\n",
        "      \n",
        "        if self.train:\n",
        "          self.req_set=train_set\n",
        "        else:\n",
        "          self.req_set=test_set\n",
        "        all_data = []  # Lista per memorizzare tutti i dati da tutti i file\n",
        "        all_imgs = []\n",
        "\n",
        "        #per le patches: dal paper\n",
        "        self.imgs_1 = {}\n",
        "        self.imgs_2 = {}\n",
        "        self.change_maps = {}\n",
        "        self.n_patches_per_image = {}\n",
        "        self.n_patches = 0\n",
        "        self.patch_coords = []\n",
        "        #per i pesi change/no-change  \n",
        "        n_pix = 0\n",
        "        true_pix = 0\n",
        "\n",
        "        for loc in self.req_set:\n",
        "            print('loading:', loc)\n",
        "            img1_list = []\n",
        "            img2_list = []\n",
        "\n",
        "            for channel in channels:\n",
        "                #prendo le immagini di ogni canale e le metto in una lista\n",
        "                ch = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/' + loc + '/imgs_1_rect/' + channel + '.tif')\n",
        "                img1_list.append(np.array(ch).reshape(ch.size[0],ch.size[1]))\n",
        "                ch = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/' + loc + '/imgs_2_rect/' + channel + '.tif')\n",
        "                img2_list.append(np.array(ch).reshape(ch.size[0],ch.size[1]))    \n",
        "            if self.train:\n",
        "                #carico l'immagine della ground truth\n",
        "                gt = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_train_labels/' + loc + '/cm/' + loc + '-cm.tif')\n",
        "            else:\n",
        "                gt = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_test_labels/' + loc + '/cm/' + loc + '-cm.tif')\n",
        "            #la ground truth ha pixel 1 (no change) e 2 (change)\n",
        "            gt = np.array(gt).reshape(gt.size[0],gt.size[1]) - 1\n",
        "            #reshape dell'immagine alle dimensioni WxHxC\n",
        "            img1=np.array(img1_list).transpose((1,2,0))\n",
        "            img2=np.array(img2_list).transpose((1,2,0))\n",
        "            if self.normalize:\n",
        "                           \n",
        "              img1=(img1-img1.mean(axis=(0,1)))/img1.std(axis=(0,1))\n",
        "              img2=(img2-img2.mean(axis=(0,1)))/img2.std(axis=(0,1))\n",
        "\n",
        "\n",
        "            #immagine diventa CxWxH come il tensore che la conterrÃ \n",
        "            img1=img1.transpose(2,0,1)\n",
        "            img2=img2.transpose(2,0,1)\n",
        "    \n",
        "            # load and store each image\n",
        "            \n",
        "            self.imgs_1[loc] = img1\n",
        "            self.imgs_2[loc] = img2\n",
        "            self.change_maps[loc] = gt\n",
        "            \n",
        "            s = gt.shape\n",
        "            n_pix += np.prod(s)\n",
        "            true_pix += gt.sum()\n",
        "            \n",
        "            # calculate the number of patches\n",
        "            s = img1.shape\n",
        "            #print(img1.shape)\n",
        "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
        "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
        "            n_patches_i = n1 * n2\n",
        "            #print(n_patches_i)\n",
        "            self.n_patches_per_image[loc] = n_patches_i\n",
        "            self.n_patches += n_patches_i\n",
        "            \n",
        "            #print(gt.shape)\n",
        "            #print(img1.shape)\n",
        "\n",
        "            # generate path coordinates\n",
        "            for i in range(n1):\n",
        "                for j in range(n2):\n",
        "                    # coordinates in (x1, x2, y1, y2)\n",
        "                    current_patch_coords = (loc, \n",
        "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side])\n",
        "                    self.patch_coords.append(current_patch_coords)\n",
        "\n",
        "        self.weights = [ self.CIBW * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
        "        return \n",
        "\n",
        "\n",
        "patch_side=96\n",
        "\n",
        "train_set_imgs=OSCD_dataset(train=True,transform=transform,stride=(int)(patch_side/2) - 1)\n",
        "test_set_imgs=OSCD_dataset(train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set_imgs, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set_imgs, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C74Niwmz3oc8",
        "outputId": "5a173de7-7aba-4138-bc36-0dafdd03dc2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading: aguasclaras\n",
            "loading: bercy\n",
            "loading: bordeaux\n",
            "loading: nantes\n",
            "loading: paris\n",
            "loading: rennes\n",
            "loading: saclay_e\n",
            "loading: abudhabi\n",
            "loading: cupertino\n",
            "loading: pisa\n",
            "loading: beihai\n",
            "loading: hongkong\n",
            "loading: beirut\n",
            "loading: mumbai\n",
            "loading: brasilia\n",
            "loading: montpellier\n",
            "loading: norcia\n",
            "loading: rio\n",
            "loading: saclay_w\n",
            "loading: valencia\n",
            "loading: dubai\n",
            "loading: lasvegas\n",
            "loading: milano\n",
            "loading: chongqing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upD8FpnNZ9C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IS_PROTOTYPE = False\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "LOAD_TRAINED = False\n",
        "\n",
        "print('DEFINITIONS OK')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWRlL2uwXCBG",
        "outputId": "e9d2fe46-3957-4e67-b318-85bcd6ea668c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEFINITIONS OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as tr\n",
        "\n",
        "'''\n",
        "# Models\n",
        "from unet import Unet\n",
        "from siamunet_conc import SiamUnet_conc\n",
        "from siamunet_diff import SiamUnet_diff\n",
        "from fresunet import FresUNet\n",
        "'''\n",
        "# Other\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from skimage import io\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm as tqdm\n",
        "from pandas import read_csv\n",
        "from math import floor, ceil, sqrt, exp\n",
        "from IPython import display\n",
        "import time\n",
        "from itertools import chain\n",
        "import time\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "\n",
        "print('IMPORTS OK')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KBycYAGWtUf",
        "outputId": "754469d8-9da4-4b50-ec53-51d46ea40bfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTS OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rodrigo Caye Daudt\n",
        "# https://rcdaudt.github.io/\n",
        "# Daudt, R. C., Le Saux, B., & Boulch, A. \"Fully convolutional siamese networks for change detection\". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    \"\"\"EF segmentation network.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
        "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "net, net_name = Unet(2*13, 2), 'FC-EF'\n"
      ],
      "metadata": {
        "id": "YBaDc0MMVg5p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Number of trainable parameters:', count_parameters(net))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxHpqfi9U-2C",
        "outputId": "c072970c-3760-4efc-a94f-fee67f61af76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 1353458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhz2pC5AXbkX",
        "outputId": "df2fff9e-c4ea-48d9-d46f-94f0665cb4c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.0.1+cu118  Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.FloatTensor(train_set_imgs.weights).to(device)\n",
        "print(weights)\n",
        "criterion = nn.NLLLoss(weight=weights) # to be used with logsoftmax output\n",
        "net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xv9LtDbYcXP",
        "outputId": "718a3980-9c12-4672-ac76-d4ced8ed89ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4578, 1.9542], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unet(\n",
              "  (conv11): Conv2d(26, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do11): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn12): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do12): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv21): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do21): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do22): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv31): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do31): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn32): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do32): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do33): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv41): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do41): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn42): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do42): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv43): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do43): Dropout2d(p=0.2, inplace=False)\n",
              "  (upconv4): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  (conv43d): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn43d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do43d): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv42d): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn42d): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do42d): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv41d): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn41d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do41d): Dropout2d(p=0.2, inplace=False)\n",
              "  (upconv3): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  (conv33d): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn33d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do33d): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv32d): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn32d): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do32d): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv31d): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn31d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do31d): Dropout2d(p=0.2, inplace=False)\n",
              "  (upconv2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  (conv22d): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn22d): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do22d): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv21d): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn21d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do21d): Dropout2d(p=0.2, inplace=False)\n",
              "  (upconv1): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  (conv12d): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn12d): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (do12d): Dropout2d(p=0.2, inplace=False)\n",
              "  (conv11d): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (sm): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs = 10, save = True):\n",
        "    t = np.linspace(1, n_epochs, n_epochs)\n",
        "    \n",
        "    epoch_train_loss = 0 * t\n",
        "    epoch_train_accuracy = 0 * t\n",
        "    epoch_train_change_accuracy = 0 * t\n",
        "    epoch_train_nochange_accuracy = 0 * t\n",
        "    epoch_train_precision = 0 * t\n",
        "    epoch_train_recall = 0 * t\n",
        "    epoch_train_Fmeasure = 0 * t\n",
        "    epoch_test_loss = 0 * t\n",
        "    epoch_test_accuracy = 0 * t\n",
        "    epoch_test_change_accuracy = 0 * t\n",
        "    epoch_test_nochange_accuracy = 0 * t\n",
        "    epoch_test_precision = 0 * t\n",
        "    epoch_test_recall = 0 * t\n",
        "    epoch_test_Fmeasure = 0 * t\n",
        "    \n",
        "#     mean_acc = 0\n",
        "#     best_mean_acc = 0\n",
        "    fm = 0\n",
        "    best_fm = 0\n",
        "    \n",
        "    lss = 1000\n",
        "    best_lss = 1000\n",
        "    \n",
        "    plt.figure(num=1)\n",
        "    plt.figure(num=2)\n",
        "    plt.figure(num=3)\n",
        "    \n",
        "    \n",
        "    optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-4)\n",
        "#     optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "        \n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
        "    \n",
        "    \n",
        "    for epoch_index in range(N_EPOCHS):\n",
        "        net.train()\n",
        "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
        "\n",
        "        tot_count = 0\n",
        "        tot_loss = 0\n",
        "        tot_accurate = 0\n",
        "        class_correct = list(0. for i in range(2))\n",
        "        class_total = list(0. for i in range(2))\n",
        "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
        "        for batch in train_loader:\n",
        "            I1 = Variable(batch['I1'].float()).to(device)\n",
        "            I2 = Variable(batch['I2'].float()).to(device)\n",
        "            label = Variable(batch['label']).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(I1, I2)\n",
        "            loss = criterion(output, label.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test(train_set_imgs)\n",
        "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
        "        \n",
        "#         epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
        "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_set_imgs)\n",
        "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
        "\n",
        "        plt.figure(num=1)\n",
        "        plt.clf()\n",
        "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
        "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
        "        plt.legend(handles=[l1_1, l1_2])\n",
        "        plt.grid()\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "        plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Loss')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=2)\n",
        "        plt.clf()\n",
        "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
        "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
        "        plt.legend(handles=[l2_1, l2_2])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=3)\n",
        "        plt.clf()\n",
        "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
        "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
        "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
        "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
        "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy per class')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=4)\n",
        "        plt.clf()\n",
        "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], label='Train precision')\n",
        "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1], label='Train recall')\n",
        "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1], label='Train Dice/F1')\n",
        "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1], label='Test precision')\n",
        "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], label='Test recall')\n",
        "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], label='Test Dice/F1')\n",
        "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 1)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Precision, Recall and F-measure')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "        \n",
        "        \n",
        "#         mean_acc = (epoch_test_nochange_accuracy[epoch_index] + epoch_test_change_accuracy[epoch_index])/2\n",
        "#         if mean_acc > best_mean_acc:\n",
        "#             best_mean_acc = mean_acc\n",
        "#             save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_acc-' + str(mean_acc) + '.pth.tar'\n",
        "#             torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        \n",
        "#         fm = pr_rec[2]\n",
        "        fm = epoch_train_Fmeasure[epoch_index]\n",
        "        if fm > best_fm:\n",
        "            best_fm = fm\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        lss = epoch_train_loss[epoch_index]\n",
        "        if lss < best_lss:\n",
        "            best_lss = lss\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "            \n",
        "            \n",
        "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
        "        if save:\n",
        "            im_format = 'png'\n",
        "    #         im_format = 'eps'\n",
        "\n",
        "            plt.figure(num=1)\n",
        "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
        "\n",
        "            plt.figure(num=2)\n",
        "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
        "\n",
        "            plt.figure(num=3)\n",
        "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
        "\n",
        "            plt.figure(num=4)\n",
        "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
        "        \n",
        "    out = {'train_loss': epoch_train_loss[-1],\n",
        "           'train_accuracy': epoch_train_accuracy[-1],\n",
        "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
        "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
        "           'test_loss': epoch_test_loss[-1],\n",
        "           'test_accuracy': epoch_test_accuracy[-1],\n",
        "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
        "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
        "    \n",
        "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
        "    print(pr_rec)\n",
        "    \n",
        "    return out\n",
        "\n",
        "L = 1024\n",
        "N = 2\n",
        "\n",
        "def test(dset):\n",
        "    net.eval()\n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    tot_accurate = 0\n",
        "    \n",
        "    n = 2\n",
        "    class_correct = list(0. for i in range(n))\n",
        "    class_total = list(0. for i in range(n))\n",
        "    class_accuracy = list(0. for i in range(n))\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    \n",
        "    if dset.train:\n",
        "      req_set=train_set\n",
        "    else:\n",
        "      req_set=test_set\n",
        "\n",
        "    for img_index in req_set:\n",
        "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
        "        \n",
        "        s = cm_full.shape\n",
        "\n",
        "        # Crea un'immagine RGB bianca delle stesse dimensioni dell'immagine di input\n",
        "        result_image = np.ones((s[0], s[1], 3), dtype=np.uint8) * 255\n",
        "       \n",
        "        print(img_index)\n",
        "        \n",
        "        steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
        "        steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
        "        for ii in range(N):\n",
        "            for jj in range(N):\n",
        "                xmin = steps0[ii]\n",
        "                if ii == N-1:\n",
        "                    xmax = s[0]\n",
        "                else:\n",
        "                    xmax = steps0[ii+1]\n",
        "                ymin = jj\n",
        "                if jj == N-1:\n",
        "                    ymax = s[1]\n",
        "                else:\n",
        "                    ymax = steps1[jj+1]\n",
        "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
        "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
        "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
        "\n",
        "                I1=torch.from_numpy(I1)\n",
        "                I2=torch.from_numpy(I2)\n",
        "\n",
        "                I1 = Variable(torch.unsqueeze(I1, 0).float()).to(device)\n",
        "                I2 = Variable(torch.unsqueeze(I2, 0).float()).to(device)\n",
        "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).to(device)\n",
        "\n",
        "\n",
        "                output = net(I1, I2)\n",
        "                loss = criterion(output, cm.long())\n",
        "        #         print(loss)\n",
        "                tot_loss += loss.data * np.prod(cm.size())\n",
        "                tot_count += np.prod(cm.size())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "                c = (predicted.int() == cm.data.int())\n",
        "                for i in range(c.size(1)):\n",
        "                    for j in range(c.size(2)):\n",
        "                        l = int(cm.data[0, i, j])\n",
        "                        class_correct[l] += c[0, i, j]\n",
        "                        class_total[l] += 1\n",
        "\n",
        "                print(predicted.shape)\n",
        "                        \n",
        "                pr = (predicted.int() > 0).cpu().numpy()\n",
        "                gt = (cm.data.int() > 0).cpu().numpy()\n",
        "\n",
        "                print(pr.shape)\n",
        "                \n",
        "                # Colora l'immagine risultato in base alla predizione\n",
        "                \n",
        "                tp += np.logical_and(pr, gt).sum()\n",
        "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
        "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
        "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
        "        \n",
        "                for i in range(gt.shape[1]):\n",
        "                  for j in range(gt.shape[2]):\n",
        "                    if (gt[0, i, j]==1 and gt[0, i, j]==pr[0, i, j]):\n",
        "                      result_image[xmin:xmax, ymin:ymax][i,j] = [255 ,255 ,255]\n",
        "                    elif (gt[0, i, j]==0 and gt[0, i, j]==pr[0, i, j]):\n",
        "                      result_image[xmin:xmax, ymin:ymax][i,j] = [0 ,0 ,0]\n",
        "                    elif (gt[0, i, j]==1 and gt[0, i, j]!=pr[0, i, j]):\n",
        "                      result_image[xmin:xmax, ymin:ymax][i,j] = [255 ,0 ,0]\n",
        "                    elif (gt[0, i, j]==0 and gt[0, i, j]!=pr[0, i, j]):\n",
        "                      result_image[xmin:xmax, ymin:ymax][i,j] = [0 ,255 ,0]\n",
        "        # Mostra l'immagine risultato\n",
        "        plt.imshow(result_image)\n",
        "        plt.axis('off')\n",
        "        plt.savefig('./'+img_index+'.png')\n",
        "\n",
        "    net_loss = tot_loss/tot_count\n",
        "    net_accuracy = 100 * (tp + tn)/tot_count\n",
        "    \n",
        "    for i in range(n):\n",
        "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    f_meas = 2 * prec * rec / (prec + rec)\n",
        "    prec_nc = tn / (tn + fn)\n",
        "    rec_nc = tn / (tn + fp)\n",
        "    \n",
        "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
        "        \n",
        "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
        "    \n",
        "    \n",
        "if LOAD_TRAINED:\n",
        "    net.load_state_dict(torch.load('net_final.pth.tar'))\n",
        "    print('LOAD OK')\n",
        "else:\n",
        "    t_start = time.time()\n",
        "    out_dic = train()\n",
        "    t_end = time.time()\n",
        "    print(out_dic)\n",
        "    print('Elapsed time:')\n",
        "    print(t_end - t_start)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "AuPlQ4_8LXq3",
        "outputId": "9693d719-b63e-42c1-9e4e-69e42e304df8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 of 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3300841e843a>\u001b[0m in \u001b[0;36m<cell line: 300>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mout_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0mt_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-3300841e843a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, save)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}