{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYkpqTZmqjn"
      },
      "outputs": [],
      "source": [
        "#Apertura dei dati da Google Drive\n",
        "#Caricamento dei file testuali per navigare nelle cartelle e prendere le immagini\n",
        "file=open(\"./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/train.txt\", \"r\")\n",
        "content = file.read()\n",
        "train_set = content.strip().split(\",\")\n",
        "print(train_set)\n",
        "file.close()\n",
        "\n",
        "file=open(\"./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/test.txt\", \"r\")\n",
        "content = file.read()\n",
        "test_set = content.strip().split(\",\")\n",
        "print(test_set)\n",
        "file.close()\n",
        "\n",
        "\n",
        "file=open(\"./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/all.txt\", \"r\")\n",
        "content = file.read()\n",
        "all_locs = content.strip().split(\",\")\n",
        "print(all_locs)\n",
        "file.close()\n",
        "\n",
        "channels = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C74Niwmz3oc8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from math import floor, ceil, sqrt, exp\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "  \n",
        "class RandomNoise():\n",
        "  def __call__(self,sample):\n",
        "    I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "    # Genera rumore salt and pepper\n",
        "    prob = 0.1  # Probabilità di avere pixel rumorosi\n",
        "    noise = np.random.choice([0, 1], size=I1.shape[1:], p=[1 - prob, prob])\n",
        "    \n",
        "    # Applica il rumore salt and pepper all'immagine\n",
        "    I1 = torch.from_numpy((I1.numpy() * (1 - noise) + noise))\n",
        "    I2 = torch.from_numpy((I2.numpy() * (1 - noise) + noise))\n",
        "    return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "class RandomFlip():\n",
        "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        if random.random() > 0.5:\n",
        "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  label.numpy()[:,::-1].copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "class RandomRot():\n",
        "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        n = random.randint(0, 3)\n",
        "        if n:\n",
        "            I1 =  sample['I1'].numpy()\n",
        "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  sample['I2'].numpy()\n",
        "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  sample['label'].numpy()\n",
        "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    # your code here\n",
        "    RandomFlip(),\n",
        "    RandomRot(),\n",
        "    RandomNoise()\n",
        "])\n",
        "\n",
        "\n",
        "#Inizilizzazione del dataset\n",
        "class OSCD_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, train=True, normalize=True, transform=None, patch_side = 96, stride = None):\n",
        "        self.train=train\n",
        "        self.transform=transform\n",
        "        self.normalize=normalize\n",
        "        self.patch_side=patch_side\n",
        "        self.stride=1\n",
        "        if stride:\n",
        "          self.stride=stride\n",
        "        #parametro utilizzato da quelli del paper per\n",
        "        #aumentare il peso dei changed pixel.\n",
        "        #potrebbe essere utile farne il tuning (?)\n",
        "       \n",
        "        #self.CIBW=10\n",
        "       \n",
        "        #Carica i dati dai file\n",
        "        self.load_data()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Restituisci la lunghezza del dataset\n",
        "        return self.n_patches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        current_patch_coords = self.patch_coords[idx]\n",
        "        loc = current_patch_coords[0]\n",
        "        limits = current_patch_coords[1]\n",
        "\n",
        "        I1 = self.imgs_1[loc][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        I2 = self.imgs_2[loc][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        label = self.change_maps[loc][limits[0]:limits[1], limits[2]:limits[3]]\n",
        "      \n",
        "\n",
        "        I1 = torch.from_numpy(I1)\n",
        "        I2 = torch.from_numpy(I2)\n",
        "        label = torch.from_numpy(label).float()\n",
        "\n",
        "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \n",
        "        return sample\n",
        "    \n",
        "    def get_img(self, loc):\n",
        "       return self.imgs_1[loc], self.imgs_2[loc], self.change_maps[loc]\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "      #selezione del sottoinsieme richiesto\n",
        "      \n",
        "        if self.train:\n",
        "          self.req_set=train_set\n",
        "        else:\n",
        "          self.req_set=test_set\n",
        "        all_data = []  # Lista per memorizzare tutti i dati da tutti i file\n",
        "        all_imgs = []\n",
        "\n",
        "        #per le patches: dal paper\n",
        "        self.imgs_1 = {}\n",
        "        self.imgs_2 = {}\n",
        "        self.change_maps = {}\n",
        "        self.n_patches_per_image = {}\n",
        "        self.n_patches = 0\n",
        "        self.patch_coords = []\n",
        "        #per i pesi change/no-change  \n",
        "        n_pix = 0\n",
        "        true_pix = 0\n",
        "\n",
        "        for loc in self.req_set:\n",
        "            print('loading:', loc)\n",
        "            img1_list = []\n",
        "            img2_list = []\n",
        "\n",
        "            for channel in channels:\n",
        "                #prendo le immagini di ogni canale e le metto in una lista\n",
        "                ch = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/' + loc + '/imgs_1_rect/' + channel + '.tif')\n",
        "                img1_list.append(np.array(ch))\n",
        "                ch = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_images/' + loc + '/imgs_2_rect/' + channel + '.tif')\n",
        "                img2_list.append(np.array(ch))    \n",
        "            if self.train:\n",
        "                #carico l'immagine della ground truth\n",
        "                gt = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_train_labels/' + loc + '/cm/' + loc + '-cm.tif')\n",
        "            else:\n",
        "                gt = Image.open('./drive/MyDrive/OSCD_dataset/OSCD_dataset_test_labels/' + loc + '/cm/' + loc + '-cm.tif')\n",
        "            #la ground truth ha pixel 1 (no change) e 2 (change)\n",
        "            #gt = np.array(gt).reshape(gt.size[0],gt.size[1]) \n",
        "            gt=np.array(gt)-1\n",
        "            '''\n",
        "            print(gt.shape)\n",
        "            plt.imshow(gt)\n",
        "            plt.show()\n",
        "            '''\n",
        "            #reshape dell'immagine alle dimensioni HxWxC\n",
        "            img1=np.array(img1_list).transpose((1,2,0))\n",
        "            img2=np.array(img2_list).transpose((1,2,0))\n",
        "            #print(img1.shape)\n",
        "            if self.normalize:\n",
        "                           \n",
        "              img1=(img1-img1.mean(axis=(0,1)))/img1.std(axis=(0,1))\n",
        "              img2=(img2-img2.mean(axis=(0,1)))/img2.std(axis=(0,1))\n",
        "\n",
        "\n",
        "            #immagine diventa CxHxW come il tensore che la conterrà\n",
        "            img1=img1.transpose(2,0,1)\n",
        "            img2=img2.transpose(2,0,1)\n",
        "    \n",
        "            # load and store each image\n",
        "            \n",
        "            self.imgs_1[loc] = img1\n",
        "            self.imgs_2[loc] = img2\n",
        "            self.change_maps[loc] = gt\n",
        "            \n",
        "            s = gt.shape\n",
        "            n_pix += np.prod(s)\n",
        "            true_pix += gt.sum()\n",
        "            \n",
        "            # calculate the number of patches\n",
        "            s = img1.shape\n",
        "            #print(img1.shape)\n",
        "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
        "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
        "            n_patches_i = n1 * n2\n",
        "            #print(n_patches_i)\n",
        "            self.n_patches_per_image[loc] = n_patches_i\n",
        "            self.n_patches += n_patches_i\n",
        "            \n",
        "            #print(gt.shape)\n",
        "            #print(img1.shape)\n",
        "\n",
        "            # generate path coordinates\n",
        "            for i in range(n1):\n",
        "                for j in range(n2):\n",
        "                    # coordinates in (x1, x2, y1, y2)\n",
        "                    current_patch_coords = (loc, \n",
        "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side])\n",
        "                    self.patch_coords.append(current_patch_coords)\n",
        "\n",
        "        #self.weights = [ self.CIBW * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
        "        #RIDEFINIZIONE PESI\n",
        "        self.weights = np.array([ 1 , (n_pix - true_pix) / true_pix])\n",
        "        self.weights = 2 * self.weights/self.weights.sum()\n",
        "        return \n",
        "\n",
        "\n",
        "patch_side=96\n",
        "\n",
        "train_set_imgs=OSCD_dataset(train=True,transform=transform,stride=(int)(patch_side/2) - 1)\n",
        "test_set_imgs=OSCD_dataset(train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set_imgs, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set_imgs, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK1XBy-eUcrS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upD8FpnNZ9C3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWRlL2uwXCBG"
      },
      "outputs": [],
      "source": [
        "IS_PROTOTYPE = False\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "LOAD_TRAINED = False\n",
        "\n",
        "print('DEFINITIONS OK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KBycYAGWtUf"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torch.autograd import Variable\n",
        "\n",
        "'''\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as tr\n",
        "'''\n",
        "'''\n",
        "# Models\n",
        "from unet import Unet\n",
        "from siamunet_conc import SiamUnet_conc\n",
        "from siamunet_diff import SiamUnet_diff\n",
        "from fresunet import FresUNet\n",
        "'''\n",
        "'''\n",
        "# Other\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from skimage import io\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm as tqdm\n",
        "from pandas import read_csv\n",
        "from math import floor, ceil, sqrt, exp\n",
        "import time\n",
        "from itertools import chain\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "'''\n",
        "import time\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "print('IMPORTS OK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBaDc0MMVg5p"
      },
      "outputs": [],
      "source": [
        "# Rodrigo Caye Daudt\n",
        "# https://rcdaudt.github.io/\n",
        "# Daudt, R. C., Le Saux, B., & Boulch, A. \"Fully convolutional siamese networks for change detection\". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    \"\"\"EF segmentation network.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "        \n",
        "        #x11 = F.relu(self.bn11(self.conv11(x)))\n",
        "        #x12 = F.relu(self.bn12(self.conv12(x11)))\n",
        "        \n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
        "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        #x21 = F.relu(self.bn21(self.conv21(x1p)))\n",
        "        #x22 = F.relu(self.bn22(self.conv22(x21)))\n",
        "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        #x31 = F.relu(self.bn31(self.conv31(x2p)))\n",
        "        #x32 = F.relu(self.bn32(self.conv32(x31)))\n",
        "        #x33 = F.relu(self.bn33(self.conv33(x32)))\n",
        "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        #x41 = F.relu(self.bn41(self.conv41(x3p)))\n",
        "        #x42 = F.relu(self.bn42(self.conv42(x41)))\n",
        "        #x43 = F.relu(self.bn43(self.conv43(x42)))\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
        "        #x43d = F.relu(self.bn43d(self.conv43d(x4d)))\n",
        "        #x42d = F.relu(self.bn42d(self.conv42d(x43d)))\n",
        "        #x41d = F.relu(self.bn41d(self.conv41d(x42d)))\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
        "        #x33d = F.relu(self.bn33d(self.conv33d(x3d)))\n",
        "        #x32d = F.relu(self.bn32d(self.conv32d(x33d)))\n",
        "        #x31d = F.relu(self.bn31d(self.conv31d(x32d)))\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
        "        #x22d = F.relu(self.bn22d(self.conv22d(x2d)))\n",
        "        #x21d = F.relu(self.bn21d(self.conv21d(x22d)))\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
        "        #x12d = F.relu(self.bn12d(self.conv12d(x1d)))\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n",
        "\n",
        "net, net_name = Unet(2*13, 2), 'FC-EF'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.hub\n",
        "\n",
        "# Definisci la tua versione personalizzata di UNet\n",
        "class TorchUnet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TorchUnet,self).__init__()\n",
        "          \n",
        "        # Mantieni gli altri livelli dal modello originale\n",
        " \n",
        "        self.model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "                     in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "        # Ridefinisci il primo livello\n",
        "        self.model.encoder1.enc1conv1 = nn.Conv2d(in_channels, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        \n",
        "        # Ridefinisci l'ultimo livello\n",
        "        self.model.conv = nn.Conv2d(32, out_channels, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "    def forward(self, i1, i2):\n",
        "        x = torch.cat((i1,i2),1)\n",
        "        #x = self.enc1conv1(x)\n",
        "        x = self.model(x)\n",
        "        #x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "USE_TORCH_NET=True\n",
        "# Utilizza il tuo modello personalizzato\n",
        "#model = TorchUNet(in_channels=3, out_channels=1)\n",
        "if USE_TORCH_NET:\n",
        "  net, net_name = TorchUnet(2*13, 2), 'Torch UNet'\n",
        "  print(net)\n"
      ],
      "metadata": {
        "id": "yf5KRjNducZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxHpqfi9U-2C"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Number of trainable parameters:', count_parameters(net))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhz2pC5AXbkX"
      },
      "outputs": [],
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xv9LtDbYcXP"
      },
      "outputs": [],
      "source": [
        "weights = torch.FloatTensor(train_set_imgs.weights).to(device)\n",
        "print(weights)\n",
        "#criterion = nn.NLLLoss(weight=weights).to(device) # to be used with logsoftmax output\n",
        "#criterion = nn.CrossEntrpyLoss(weight=weights).to(device) # to be used with logsoftmax output\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "  def __init__(self,gamma=10,alpha=0.05,reduction='mean'):\n",
        "    super(FocalLoss, self).__init__()\n",
        "    self.gamma=gamma\n",
        "    self.alpha=alpha\n",
        "    self.reduction=reduction\n",
        "  def forward(self,output,label):\n",
        "    pt=torch.exp(output)\n",
        "    #Output NxCxHxW -> NxCxH*W\n",
        "    output=output.view(output.shape[0],output.shape[1],-1)\n",
        "    pt=pt.view(pt.shape[0],pt.shape[1],-1)\n",
        "    #Label NxHxW -> NxH*W\n",
        "    label=label.view(label.shape[0],-1)\n",
        "\n",
        "    #Output NxCxH*W -> NxH*WxC -> N*H*WxC\n",
        "    #output=output.transpose(1,2)\n",
        "    #pt=pt.transpose(1,2)\n",
        "\n",
        "    #output=output.contiguous().view(-1,output.shape[2])\n",
        "    #pt=pt.contiguous().view(-1,pt.shape[2])\n",
        "\n",
        "    output=output.transpose(0,1)\n",
        "    pt=pt.transpose(0,1)\n",
        "    #output=output.squeeze(0)\n",
        "    #pt=pt.squeeze(0)\n",
        "    #Label NxH*W -> N*H*W\n",
        "    #label=label.view(-1)\n",
        "\n",
        "    no_change_loss=-((1-label)*self.alpha*((1-pt[0])**self.gamma)*output[0]).sum(axis=1)\n",
        "    change_loss=-((label)*(1-self.alpha)*((1-pt[1])**self.gamma)*output[1]).sum(axis=1)\n",
        "    #print(pt[0]+pt[1])\n",
        "    #print('change',change_loss.mean())\n",
        "    #print('no_change',no_change_loss.mean())\n",
        "    focal_loss=change_loss+no_change_loss\n",
        "    #print(focal_loss.shape)\n",
        "    if self.reduction == 'mean':\n",
        "        focal_loss = torch.mean(focal_loss)\n",
        "    elif self.reduction == 'sum':\n",
        "        focal_loss = torch.sum(focal_loss)\n",
        "    return focal_loss\n",
        "\n",
        "criterion = FocalLoss().to(device)\n",
        "    \n",
        "\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjGlKnveBRVl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AuPlQ4_8LXq3"
      },
      "outputs": [],
      "source": [
        "def train(n_epochs = 10, save = True):\n",
        "    t = np.linspace(1, n_epochs, n_epochs)\n",
        "    \n",
        "    epoch_train_loss = 0 * t\n",
        "    epoch_train_accuracy = 0 * t\n",
        "    epoch_train_change_accuracy = 0 * t\n",
        "    epoch_train_nochange_accuracy = 0 * t\n",
        "    epoch_train_precision = 0 * t\n",
        "    epoch_train_recall = 0 * t\n",
        "    epoch_train_Fmeasure = 0 * t\n",
        "    epoch_test_loss = 0 * t\n",
        "    epoch_test_accuracy = 0 * t\n",
        "    epoch_test_change_accuracy = 0 * t\n",
        "    epoch_test_nochange_accuracy = 0 * t\n",
        "    epoch_test_precision = 0 * t\n",
        "    epoch_test_recall = 0 * t\n",
        "    epoch_test_Fmeasure = 0 * t\n",
        "    \n",
        "#     mean_acc = 0\n",
        "#     best_mean_acc = 0\n",
        "    fm = 0\n",
        "    best_fm = 0\n",
        "    \n",
        "    lss = 1000\n",
        "    best_lss = 1000\n",
        "    \n",
        "    plt.figure(num=1)\n",
        "    plt.figure(num=2)\n",
        "    plt.figure(num=3)\n",
        "    \n",
        "    \n",
        "    optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-4)\n",
        "#     optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "        \n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
        "    \n",
        "    \n",
        "    for epoch_index in range(N_EPOCHS):\n",
        "        net.train()\n",
        "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
        "\n",
        "        tot_count = 0\n",
        "        tot_loss = 0\n",
        "        tot_accurate = 0\n",
        "        class_correct = list(0. for i in range(2))\n",
        "        class_total = list(0. for i in range(2))\n",
        "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
        "        for batch in train_loader:\n",
        "            I1 = Variable(batch['I1'].float()).to(device)\n",
        "            I2 = Variable(batch['I2'].float()).to(device)\n",
        "            label = Variable(batch['label']).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(I1, I2)\n",
        "            loss = criterion(output, label.long())\n",
        "            #print(loss.item(),loss.mean(),loss.sum())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test(train_set_imgs)\n",
        "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
        "        \n",
        "#         epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
        "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_set_imgs)\n",
        "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
        "\n",
        "        plt.figure(num=1)\n",
        "        plt.clf()\n",
        "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
        "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
        "        plt.legend(handles=[l1_1, l1_2])\n",
        "        plt.grid()\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "        plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Loss')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=2)\n",
        "        plt.clf()\n",
        "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
        "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
        "        plt.legend(handles=[l2_1, l2_2])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy')\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=3)\n",
        "        plt.clf()\n",
        "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
        "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
        "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
        "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
        "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy per class')\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=4)\n",
        "        plt.clf()\n",
        "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], label='Train precision')\n",
        "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1], label='Train recall')\n",
        "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1], label='Train Dice/F1')\n",
        "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1], label='Test precision')\n",
        "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], label='Test recall')\n",
        "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], label='Test Dice/F1')\n",
        "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 1)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Precision, Recall and F-measure')\n",
        "        display.display(plt.gcf())\n",
        "        \n",
        "        \n",
        "#         mean_acc = (epoch_test_nochange_accuracy[epoch_index] + epoch_test_change_accuracy[epoch_index])/2\n",
        "#         if mean_acc > best_mean_acc:\n",
        "#             best_mean_acc = mean_acc\n",
        "#             save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_acc-' + str(mean_acc) + '.pth.tar'\n",
        "#             torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        \n",
        "#         fm = pr_rec[2]\n",
        "        fm = epoch_train_Fmeasure[epoch_index]\n",
        "        if fm > best_fm:\n",
        "            best_fm = fm\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        lss = epoch_train_loss[epoch_index]\n",
        "        if lss < best_lss:\n",
        "            best_lss = lss\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "            \n",
        "            \n",
        "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
        "        if save:\n",
        "            im_format = 'png'\n",
        "    #         im_format = 'eps'\n",
        "\n",
        "            plt.figure(num=1)\n",
        "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
        "\n",
        "            plt.figure(num=2)\n",
        "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
        "\n",
        "            plt.figure(num=3)\n",
        "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
        "\n",
        "            plt.figure(num=4)\n",
        "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
        "        \n",
        "    out = {'train_loss': epoch_train_loss[-1],\n",
        "           'train_accuracy': epoch_train_accuracy[-1],\n",
        "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
        "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
        "           'test_loss': epoch_test_loss[-1],\n",
        "           'test_accuracy': epoch_test_accuracy[-1],\n",
        "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
        "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
        "    \n",
        "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
        "    print(pr_rec)\n",
        "    \n",
        "    return out\n",
        "\n",
        "L = 1024\n",
        "N = 2\n",
        "\n",
        "def test(dset):\n",
        "\n",
        "    net.eval()\n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    tot_accurate = 0\n",
        "    \n",
        "    n = 2\n",
        "    class_correct = list(0. for i in range(n))\n",
        "    class_total = list(0. for i in range(n))\n",
        "    class_accuracy = list(0. for i in range(n))\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    \n",
        "    if dset.train:\n",
        "      req_set=train_set\n",
        "    else:\n",
        "      req_set=test_set\n",
        "\n",
        "    for img_index in req_set:\n",
        "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
        "        \n",
        "        s = cm_full.shape\n",
        "\n",
        "        # Crea un'immagine RGB bianca delle stesse dimensioni dell'immagine di input\n",
        "        result_image = np.zeros((s[0], s[1], 3), dtype=np.uint8) \n",
        "\n",
        "       \n",
        "        print(img_index)\n",
        "        if USE_TORCH_NET:\n",
        "            N=16\n",
        "            steps0 = np.arange(0,s[0],N)\n",
        "            steps1 = np.arange(0,s[1],N)\n",
        "            N1=ceil(s[0]/N)-1\n",
        "            N2=ceil(s[1]/N)-1\n",
        "        else:\n",
        "            steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
        "            steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
        "            N1=N\n",
        "            N2=N\n",
        "        for ii in range(N1):\n",
        "            for jj in range(N2):\n",
        "                if USE_TORCH_NET:\n",
        "                    xmin = steps0[ii]\n",
        "                   \n",
        "                    xmax = steps0[ii+1]\n",
        "                    ymin = steps1[jj]\n",
        "            \n",
        "                    ymax = steps1[jj+1]                    \n",
        "                    \n",
        "                else: \n",
        "                    xmin = steps0[ii]\n",
        "                    if ii == N-1:\n",
        "                        xmax = s[0]\n",
        "                    else:\n",
        "                        xmax = steps0[ii+1]\n",
        "                    ymin = steps1[jj]\n",
        "                    if jj == N-1:\n",
        "                        ymax = s[1]\n",
        "                    else:\n",
        "                        ymax = steps1[jj+1]\n",
        "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
        "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
        "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
        "\n",
        "                #print(cm_full.shape)\n",
        "\n",
        "                I1=torch.from_numpy(I1)\n",
        "                I2=torch.from_numpy(I2)\n",
        "\n",
        "                I1 = Variable(torch.unsqueeze(I1, 0).float()).to(device)\n",
        "                I2 = Variable(torch.unsqueeze(I2, 0).float()).to(device)\n",
        "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).to(device)\n",
        "\n",
        "\n",
        "                output = net(I1, I2)\n",
        "                loss = criterion(output, cm.long())\n",
        "        #         print(loss)\n",
        "                tot_loss += loss.data * np.prod(cm.size())\n",
        "                tot_count += np.prod(cm.size())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "                c = (predicted.int() == cm.data.int())\n",
        "                for i in range(c.size(1)):\n",
        "                    for j in range(c.size(2)):\n",
        "                        l = int(cm.data[0, i, j])\n",
        "                        class_correct[l] += c[0, i, j]\n",
        "                        class_total[l] += 1\n",
        "   \n",
        "                pr = (predicted.int() > 0).cpu().numpy()\n",
        "                gt = (cm.data.int() > 0).cpu().numpy()\n",
        "                \n",
        "                #print(pr.shape)\n",
        "                #print(cm.shape)\n",
        "                #print(xmin,xmax,ymin,ymax)\n",
        "                # Colora l'immagine risultato in base alla predizione\n",
        "\n",
        "                pr=pr[0]\n",
        "                gt=gt[0]\n",
        "\n",
        "                pr_and_gt=np.logical_and(pr, gt)\n",
        "                not_pr_and_not_gt=np.logical_and(np.logical_not(pr), np.logical_not(gt))\n",
        "                pr_and_not_gt=np.logical_and(pr, np.logical_not(gt))\n",
        "                not_pr_and_gt=np.logical_and(np.logical_not(pr), gt)\n",
        "                \n",
        "                tp += pr_and_gt.sum()\n",
        "                tn += not_pr_and_not_gt.sum()\n",
        "                fp +=pr_and_not_gt.sum()\n",
        "                fn += not_pr_and_gt.sum()\n",
        "                \n",
        "              \n",
        "                result_image[xmin:xmax,ymin:ymax][pr_and_gt] = [255, 255, 255]\n",
        "                result_image[xmin:xmax,ymin:ymax][not_pr_and_not_gt] = [0, 0, 0]\n",
        "                result_image[xmin:xmax,ymin:ymax][pr_and_not_gt] = [255, 0, 0]\n",
        "                result_image[xmin:xmax,ymin:ymax][not_pr_and_gt] = [0, 255, 0]\n",
        "                \n",
        "        # Mostra l'immagine risultato\n",
        "        result_image = Image.fromarray(result_image)\n",
        "        result_image.show()\n",
        "        result_image.save(img_index+'.png')\n",
        "\n",
        "    net_loss = tot_loss/tot_count\n",
        "    net_accuracy = 100 * (tp + tn)/tot_count\n",
        "    \n",
        "    for i in range(n):\n",
        "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    f_meas = 2 * prec * rec / (prec + rec)\n",
        "    prec_nc = tn / (tn + fn)\n",
        "    rec_nc = tn / (tn + fp)\n",
        "    \n",
        "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
        "        \n",
        "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
        "    \n",
        "    \n",
        "if LOAD_TRAINED:\n",
        "    net.load_state_dict(torch.load('net_final.pth.tar'))\n",
        "    print('LOAD OK')\n",
        "else:\n",
        "    t_start = time.time()\n",
        "    out_dic = train()\n",
        "    t_end = time.time()\n",
        "    print(out_dic)\n",
        "    print('Elapsed time:')\n",
        "    print(t_end - t_start)\n",
        "    \n",
        "from google.colab import files\n",
        "\n",
        "for loc in all_locs:\n",
        "  files.download(loc+'.png')\n",
        "  \n",
        "files.download('FC-EF-01-loss.png')\n",
        "files.download('FC-EF-02-accuracy.png')\n",
        "files.download('FC-EF-03-accuracy-per-class.png')\n",
        "files.download('FC-EF-04-prec-rec-fmeas.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LK6LogmLGka5"
      },
      "outputs": [],
      "source": [
        "files.download('FC-EF-04-prec-rec-fmeas.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h1QQRwpTvrz_"
      },
      "outputs": [],
      "source": [
        "\n",
        "for loc in all_locs:\n",
        "  files.download(loc+'.png')\n",
        "  \n",
        "files.download('FC-EF-01-loss.png')\n",
        "files.download('FC-EF-02-accuracy.png')\n",
        "files.download('FC-EF-03-accuracy-per-class.png')\n",
        "files.download('FC-EF-04-prec-rec-fmeas.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}